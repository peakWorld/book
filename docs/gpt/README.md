# GPT
全称Generative Pre-trained Transformer

编码器-解码器是两个不同的模型，就像你看视频，你的手机上并不需要视频录制、编辑软件，只需要一个解码-播放器一样。训练两个模型太麻烦，我们希望就围绕一个模型做研究，做训练。我们能不能不做编码，就围绕解码模型来达到一些目的呢？答案当然是可以的。
打个不严谨的比方。我现在想找人帮我画一幅肖像画。其实我并不懂怎么画画。于是，我请人给我画。我并不能从画工技艺、艺术审美方面去评判他画得好不好。但是，我是有能力去判断我请的人画出来的画是不是令我满意的。此时，我就是一个decode-only的模型。你会说，“你这个decode-only的模型必须要有一个懂encode的画师才能工作啊“。是的，我不懂画画。确实需要一个画师。但是，你记得吗，OpenAI训练GPT3的模型，就是给它海量的数据，让它去训练。那么，画师不教导我绘画技巧，只是不停的给我画肖像，并且，给我看肖像是否满意，我指出哪些地方不满意，他做出修改。这个事情干了一千万次，我是不是至少能学到‘当给我一副没画好的我的肖像画，我知道应该怎么接着画下一笔‘？我不是从拆解好的理论体系里去学习的，我并不能叫出各种会画技法的名字，但是，我就是会做上面这件事情了。相当于，我听到“GPT是一个预训练模”，我就知道下一个字一定是“型”字一样。而因为我只擅长接着前面做好的事情完成接下来的事情，所以，我会‘生成’这个工作方式，同时，我也只会‘生成’这个工作方式。这就是Generative的意思。

总结一下，Generative是被训练出来的模型的工作的方式，Transformer是这个模型的架构，Pre-trained是形容Transformer的，就是说训练这个模型，预训练是不可或缺的核心步骤。

# 指令对齐
通过一些‘问题-回答’对的训练数据，让模型能在收到“给我写一段简介”这样的指令性的输入的时候，真地去按照指令的要求去生成接下来的东西;而不是把你说的话续写下去。

# 强化学习
经历了预训练、指令对齐、你可以理解为，一个大学生上一门课，指令对齐就是老师在课堂上给我们上课。但是，我们学习不能总是耗着一个老师一直给我们讲啊，成本太高了。就是说，积累大量指令对齐的QA（问题-回答）训练数据，成本很高。后续更大量的学习，还得学生自己在脱离老师的强帮助，自行学习。

用完成了指令对齐的模型，针对大量问题作出回答。并且，每个问题都给出10个回答。由人类去标注，所有回答里，哪些回答更好。用这个标注的QA对去接着‘对齐’模型，让模型把知识对齐到更符合我们的要求。这个时候，人类需要做的，就仅仅是提问题+标注，之前是要给出问题且给出答案。

# LoRA与instruction fine-tuning
在有一个已经训练好的大模型之后，再训练一个依赖于大模型的小模型，组合在一起工作，达到用较低的成本实现对大模型的微调。结果稍稍裂化于对大模型进行了微调，但是微调成本更低。

* LoRA
预先定义的规则和知识被用于引导模型的学习过程，从而提高模型在特定任务上的性能。使模型在训练过程中利用这些规则，从而更好地理解和处理特定领域的问题。这种方法通常用于那些需要专业知识或领域知识的任务，如法律、医学或金融等。

* Instruction fine-tuning（指令微调）
在包含用户指令及其期望结果的数据集上训练模型，帮助模型更好地理解用户输入背后的意图，并生成更准确、更相关的响应。指令微调通常在模型经过大型数据集预训练之后进行，目的是使模型适应特定任务或一组指令。


## 学习资料
* [吴恩达<斯坦福 CS229 机器学习>课程](https://www.bilibili.com/video/BV1JE411w7Ub/?vd_source=45ec08ffb275ecf5a715685d67e52040)
* [<斯坦福 CS224n 自然语言处理>课程](https://www.bilibili.com/video/BV1SL411U7SF/?vd_source=45ec08ffb275ecf5a715685d67e52040)
* [<伯克利 CS285 | 深度强化学习>课程](https://www.bilibili.com/video/BV12341167kL/?vd_source=45ec08ffb275ecf5a715685d67e52040)
* [新手友好的入门路径:李宏毅机器学习系列](https://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html)
大概学懂前面的内容之后，看深度学习二号人物bengio写的书，大家亲切地叫它‘花书’。
* [深度学习, 二号人物bengio写的书](https://book.douban.com/subject/27087503/)
* [动手学深度学习（PyTorch版）, PyTorch的API极度友好，通过下面这本书来数学。](https://book.douban.com/subject/36142067/)
很多概念理解半天都理解不了，特别是统计学的概念，在碰到的时候，就可以去B站找解释得比较好的视频，
* [十分钟搞定最大似然估计](https://www.bilibili.com/video/BV1Hb4y1m7rE/?vd_source=45ec08ffb275ecf5a715685d67e52040)
* [【概率论】贝叶斯公式与后验概率](https://www.bilibili.com/video/BV1mY411h7ps/?vd_source=45ec08ffb275ecf5a715685d67e52040)
* [极大似然估计/最大后验估计—通过抛硬币例子理解](https://www.bilibili.com/video/BV1GZ4y1m7gv/?vd_source=45ec08ffb275ecf5a715685d67e52040)
* [「一个模型」教你搞定贝叶斯和全概率公式](https://www.bilibili.com/video/BV1a4411B7B4/?vd_source=45ec08ffb275ecf5a715685d67e52040)
* [“损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法”](https://www.bilibili.com/video/BV1Y64y1Q7hi/?vd_source=45ec08ffb275ecf5a715685d67e52040)
* [“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”](https://www.bilibili.com/video/BV1Y64y1Q7hi/?vd_source=45ec08ffb275ecf5a715685d67e52040)
* [重新理解线性回归 - 2 - 广义线性模型：sigmoid函数到底是怎么来的](https://www.bilibili.com/video/BV13X4y1R7im/?vd_source=45ec08ffb275ecf5a715685d67e52040)

## 文章
* https://km.woa.com/articles/show/578524?kmref=search&from_page=1&no=6